{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import gc\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n",
    "from effdet.efficientdet import HeadNet\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_ROOT_PATH = 'test_images'\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictGlobalConfig:\n",
    "    restore_from = '../experiments/effdet5_ds_only_impact_master_22_12_2020/best-checkpoint-007epoch.bin'\n",
    "    num_workers = 4\n",
    "    batch_size = 3\n",
    "    \n",
    "    #folder = '../experiments/effdet5_ds_only_impact_master_22_12_2020_ft'\n",
    "    soft_nms = True\n",
    "    \n",
    "predict_config = PredictGlobalConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, image_ids, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_ids = image_ids\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'../data/train_images_all/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "        return image, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(images, score_threshold=0.5, all_boxes=False):\n",
    "    images = torch.stack(images).to(DEVICE).float()\n",
    "    box_list = []\n",
    "    score_list = []\n",
    "    with torch.no_grad():\n",
    "        det = net(images)#, torch.tensor([1]*images.shape[0]).float().cuda())\n",
    "        for i in range(images.shape[0]):\n",
    "            boxes = det[i].detach().cpu().numpy()[:,:4]    \n",
    "            scores = det[i].detach().cpu().numpy()[:,4]   \n",
    "            label = det[i].detach().cpu().numpy()[:,5]\n",
    "            # useing only label = 2\n",
    "            if all_boxes:\n",
    "                indexes = np.where((scores > score_threshold) )[0]\n",
    "            else:\n",
    "                indexes = np.where((scores > score_threshold) & (label == 2))[0]\n",
    "            #boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "            #boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "            box_list.append(boxes[indexes])\n",
    "            score_list.append(scores[indexes])\n",
    "    return box_list, score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:1') \n",
    "SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(predict_config):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    config.image_size = [512, 512]\n",
    "    config.norm_kwargs = dict(eps=0.001, momentum=0.01)\n",
    "    config.soft_nms = predict_config.soft_nms\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    checkpoint = torch.load(predict_config.restore_from, map_location=DEVICE)\n",
    "    \n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "    \n",
    "    net.reset_head(num_classes=2)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    net = DetBenchPredict(net)\n",
    "    net.eval();\n",
    "    return net.to(DEVICE)\n",
    "#if IS_PRIVATE:\n",
    "net = load_net(predict_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('../data/evaluation_labels.csv')\n",
    "imgs = val_df.image_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.loc[(val_df.impact == 2) & (val_df.confidence <= 1), 'impact'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.loc[(val_df.impact == 2) & (val_df.visibility <= 0), 'impact'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetRetriever(\n",
    "    image_ids=np.array(imgs),\n",
    "    transforms=get_valid_transforms()\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.efficientnet import tf_efficientnet_b4_ns, tf_efficientnet_b3_ns, \\\n",
    "    tf_efficientnet_b5_ns, tf_efficientnet_b2_ns, tf_efficientnet_b6_ns, tf_efficientnet_b7_ns\n",
    "from nfl_impact_detection.scripts.classifier.model import ImpactClassifier\n",
    "class PredictionClassifierConfig:\n",
    "    restore_from = '../experiments/classifiers/effnet7_baseline_aug_3/best-checkpoint-005epoch.bin'\n",
    "    num_workers = 4\n",
    "    margin = 2.5\n",
    "    encoder = {\n",
    "                \"name\" : tf_efficientnet_b7_ns,\n",
    "                \"features\": 2560\n",
    "              }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_classifier_transforms():\n",
    "    return A.Compose(\n",
    "        [   \n",
    "            A.Resize(height=128, width=128, p=1),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_config = PredictionClassifierConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ImpactClassifier(encoder=classifier_config.encoder)\n",
    "\n",
    "checkpoint = torch.load(classifier_config.restore_from,  map_location=DEVICE)\n",
    "classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "classifier.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_crop_size_by_box(box):\n",
    "    x, y, w, h = box\n",
    "    crop_size = max(w, h) // 2\n",
    "    crop_size += crop_size * classifier_config.margin\n",
    "    crop_size = round(crop_size)\n",
    "    return crop_size\n",
    "\n",
    "def get_cropped_samples(image_idx, boxes, scores, transforms=None):\n",
    "    image = cv2.imread(f'../data/train_images_all/{image_idx}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    \n",
    "    crop_sizes = [get_crop_size_by_box(box) for box in boxes]\n",
    "    max_crop_size = max(crop_sizes)\n",
    "\n",
    "    im_size = image.shape\n",
    "    pad_image = np.zeros((im_size[0]+2*max_crop_size, im_size[1]+2*max_crop_size, im_size[2]))\n",
    "    pad_image[max_crop_size:pad_image.shape[0]-max_crop_size, \n",
    "            max_crop_size:pad_image.shape[1]-max_crop_size] = image\n",
    "\n",
    "    outputs = []\n",
    "    for i, box in enumerate(boxes):\n",
    "        crop_size = crop_sizes[i]\n",
    "        x, y, w, h = box\n",
    "        center = [0,0]\n",
    "        center[0] = x + w // 2 + max_crop_size\n",
    "        center[1] = y + h // 2 + max_crop_size\n",
    "\n",
    "        crop_img = pad_image[center[1]-crop_size:center[1]+crop_size,\n",
    "                        center[0]-crop_size:center[0]+crop_size].astype('uint8')\n",
    "        if transforms:\n",
    "            sample = transforms(**{\n",
    "                'image': crop_img\n",
    "            })     \n",
    "        sample['box'] = box\n",
    "        sample['detector_score'] = scores[i]\n",
    "        outputs.append(sample)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=653.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ee44346b0d5447699f50c191d872ccf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2031c0a88a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 )\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mclassifier_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtta_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#flip(2) for horizontal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;31m#classifier_preds2 = torch.sigmoid(classifier(torch.stack([sample['image'].flip(2) for sample in cropped_samples]))).detach().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcropped_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/nfl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/nfl_impact_detection/scripts/classifier/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/nfl/lib/python3.6/site-packages/timm/models/efficientnet.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/nfl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/nfl/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/nfl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/nfl/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/nfl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/nfl/lib/python3.6/site-packages/timm/models/efficientnet_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m# Squeeze-and-excitation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m# Point-wise linear projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/nfl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/nfl/lib/python3.6/site-packages/timm/models/efficientnet_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mx_se\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_se\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mx_se\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_se\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_se\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "all_boxes=False\n",
    "result_image_ids = []\n",
    "results_boxes = []\n",
    "results_scores = []\n",
    "results_detector_scores = []\n",
    "results_classifier_scores = []\n",
    "for images, image_ids in tqdm.tqdm_notebook(data_loader):\n",
    "    box_list, score_list = make_predictions(images, score_threshold=0.3, all_boxes=all_boxes)\n",
    "    #predictions = make_tta_predictions(images)\n",
    "    for i, image in enumerate(images):\n",
    "        #boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "\n",
    "        #score_threshold = STAGE1_SUPPRESS\n",
    "        #indexes = np.where(scores>score_threshold)\n",
    "        #boxes = boxes[indexes]\n",
    "        #scores = scores[indexes]\n",
    "        \n",
    "        boxes = box_list[i]\n",
    "        scores = score_list[i]\n",
    "        detector_scores = np.array([])\n",
    "        classifier_scores = np.array([])\n",
    "        image_id = image_ids[i]\n",
    "    \n",
    "        boxes[:, 0] = (boxes[:, 0] * 1280 / 512)\n",
    "        boxes[:, 1] = (boxes[:, 1] * 720 / 512)\n",
    "        boxes[:, 2] = (boxes[:, 2] * 1280 / 512)\n",
    "        boxes[:, 3] = (boxes[:, 3] * 720 / 512)\n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        boxes = boxes.astype(np.int32)\n",
    "        boxes[:, 0] = boxes[:, 0].clip(min=0, max=1280-1)\n",
    "        boxes[:, 2] = boxes[:, 2].clip(min=0, max=1280-1)\n",
    "        boxes[:, 1] = boxes[:, 1].clip(min=0, max=720-1)\n",
    "        boxes[:, 3] = boxes[:, 3].clip(min=0, max=720-1)\n",
    "        \n",
    "        if len(boxes)>0:    \n",
    "            cropped_samples = get_cropped_samples(\n",
    "                image_id,\n",
    "                boxes,\n",
    "                scores,\n",
    "                transforms=get_valid_classifier_transforms())\n",
    "            \n",
    "            boxes=[]\n",
    "            scores=[]\n",
    "            classifier_scores=[]\n",
    "            detector_scores=[]\n",
    "            tta_samples = torch.stack(\n",
    "                [sample['image'] for sample in cropped_samples] +\n",
    "                [sample['image'].flip(0) for sample in cropped_samples] +\n",
    "                [sample['image'].flip(2) for sample in cropped_samples] +\n",
    "                [sample['image'].flip(0).flip(2) for sample in cropped_samples]\n",
    "                )\n",
    "\n",
    "            classifier_preds = torch.sigmoid(classifier(tta_samples)).detach().numpy() #flip(2) for horizontal\n",
    "            #classifier_preds2 = torch.sigmoid(classifier(torch.stack([sample['image'].flip(2) for sample in cropped_samples]))).detach().numpy()\n",
    "            for i, sample in enumerate(cropped_samples):\n",
    "                #classifier_score = torch.sigmoid(classifier(sample['image'])).detach().numpy()\n",
    "                l = len(cropped_samples)\n",
    "                classifier_score = (2*classifier_preds[i]+classifier_preds[i+l]+classifier_preds[i+l*2]+classifier_preds[i+l*3]) / 5.\n",
    "                boxes.append(sample['box'])\n",
    "                classifier_scores.append(classifier_score[0])\n",
    "                detector_scores.append(sample['detector_score'])\n",
    "                scores.append(classifier_score[0] * sample['detector_score'])\n",
    "\n",
    "            boxes = np.array(boxes)\n",
    "            scores = np.array(scores)\n",
    "            detector_scores = np.array(detector_scores)\n",
    "            classifier_scores = np.array(classifier_scores)\n",
    "\n",
    "        results_boxes.append(boxes)\n",
    "        results_scores.append(scores)\n",
    "        results_detector_scores.append(detector_scores)\n",
    "        results_classifier_scores.append(classifier_scores)\n",
    "        result_image_ids += [image_id]*len(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1910, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 204
    }
   ],
   "source": [
    "box_df = pd.DataFrame(np.concatenate(results_boxes), columns=['left', 'top', 'width', 'height'])\n",
    "test_df = pd.DataFrame({'scores':np.concatenate(results_scores), 'image_name':result_image_ids})\n",
    "test_df_detector = pd.DataFrame({'detector_scores':np.concatenate(results_detector_scores)})\n",
    "test_df_classifier = pd.DataFrame({'classifier_scores':np.concatenate(results_classifier_scores)})\n",
    "\n",
    "test_df = pd.concat([test_df, test_df_detector, test_df_classifier, box_df], axis=1)\n",
    "test_df['frame'] = test_df.image_name.apply(lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "test_df['video'] = test_df.image_name.apply(lambda x: '_'.join(x.split('_')[:3])+'.mp4' )\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     scores                   image_name  detector_scores  classifier_scores  \\\n",
       "0  0.168355  57583_000082_Endzone_35.png         0.215000           0.783049   \n",
       "1  0.183240  57583_000082_Endzone_36.png         0.213280           0.859153   \n",
       "2  0.076194  57583_000082_Endzone_40.png         0.203800           0.373868   \n",
       "3  0.167738  57583_000082_Endzone_42.png         0.221987           0.755618   \n",
       "4  0.202959  57583_000082_Endzone_43.png         0.276977           0.732766   \n",
       "\n",
       "   left  top  width  height  frame                     video  \n",
       "0   564  326     21      21     35  57583_000082_Endzone.mp4  \n",
       "1   564  326     21      20     36  57583_000082_Endzone.mp4  \n",
       "2   443  320     21      17     40  57583_000082_Endzone.mp4  \n",
       "3   442  319     20      16     42  57583_000082_Endzone.mp4  \n",
       "4   443  320     19      15     43  57583_000082_Endzone.mp4  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>scores</th>\n      <th>image_name</th>\n      <th>detector_scores</th>\n      <th>classifier_scores</th>\n      <th>left</th>\n      <th>top</th>\n      <th>width</th>\n      <th>height</th>\n      <th>frame</th>\n      <th>video</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.168355</td>\n      <td>57583_000082_Endzone_35.png</td>\n      <td>0.215000</td>\n      <td>0.783049</td>\n      <td>564</td>\n      <td>326</td>\n      <td>21</td>\n      <td>21</td>\n      <td>35</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.183240</td>\n      <td>57583_000082_Endzone_36.png</td>\n      <td>0.213280</td>\n      <td>0.859153</td>\n      <td>564</td>\n      <td>326</td>\n      <td>21</td>\n      <td>20</td>\n      <td>36</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.076194</td>\n      <td>57583_000082_Endzone_40.png</td>\n      <td>0.203800</td>\n      <td>0.373868</td>\n      <td>443</td>\n      <td>320</td>\n      <td>21</td>\n      <td>17</td>\n      <td>40</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.167738</td>\n      <td>57583_000082_Endzone_42.png</td>\n      <td>0.221987</td>\n      <td>0.755618</td>\n      <td>442</td>\n      <td>319</td>\n      <td>20</td>\n      <td>16</td>\n      <td>42</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.202959</td>\n      <td>57583_000082_Endzone_43.png</td>\n      <td>0.276977</td>\n      <td>0.732766</td>\n      <td>443</td>\n      <td>320</td>\n      <td>19</td>\n      <td>15</td>\n      <td>43</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 205
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(scores, frames, x1, y1, x2, y2, thresh=0.5, fr_th=200):\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "        frm = frames[order[1:]]\n",
    "\n",
    "        inds = np.where((ovr <= thresh) | (abs(frm - frames[i]) > fr_th))[0]\n",
    "        # inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_nms(test_df, tr=0.5, fr_tr=200):\n",
    "    indices = []\n",
    "    leave = 0\n",
    "    for vid in test_df.video.unique():\n",
    "        info = test_df[test_df.video ==vid]\n",
    "        #print(info.columns)\n",
    "        x1 = info['left'].values\n",
    "        y1 = info['top'].values\n",
    "        x2 = x1 + info['width'].values\n",
    "        y2 = y1 + info['height'].values\n",
    "        res = nms(info['scores'].values, info['frame'].values, x1, y1, x2, y2, tr, fr_tr)\n",
    "        leave += len(res)\n",
    "        #print(vid,' to_drop', info.shape[0]-len(res))\n",
    "        indices_to_drop = info.index[list(set(list(range(info.shape[0]))) - set(res))]\n",
    "        indices.extend(list(indices_to_drop))\n",
    "    test_df= test_df.drop(index = indices)\n",
    "    #print(leave, len(indices))\n",
    "    return test_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(bbox1, bbox2):\n",
    "    bbox1 = [float(x) for x in bbox1]\n",
    "    bbox2 = [float(x) for x in bbox2]\n",
    "\n",
    "    (x0_1, y0_1, x1_1, y1_1) = bbox1\n",
    "    (x0_2, y0_2, x1_2, y1_2) = bbox2\n",
    "\n",
    "    # get the overlap rectangle\n",
    "    overlap_x0 = max(x0_1, x0_2)\n",
    "    overlap_y0 = max(y0_1, y0_2)\n",
    "    overlap_x1 = min(x1_1, x1_2)\n",
    "    overlap_y1 = min(y1_1, y1_2)\n",
    "\n",
    "    # check if there is an overlap\n",
    "    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n",
    "        return 0\n",
    "\n",
    "    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n",
    "    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n",
    "    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n",
    "    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n",
    "    size_union = size_1 + size_2 - size_intersection\n",
    "\n",
    "    return size_intersection / size_union\n",
    "\n",
    "\n",
    "def precision_calc(gt_boxes, pred_boxes):\n",
    "    cost_matix = np.ones((len(gt_boxes), len(pred_boxes)))\n",
    "    for i, box1 in enumerate(gt_boxes):\n",
    "        for j, box2 in enumerate(pred_boxes):\n",
    "            dist = abs(box1[0] - box2[0])\n",
    "            if dist > 4:\n",
    "                continue\n",
    "            iou_score = iou(box1[1:], box2[1:])\n",
    "\n",
    "            if iou_score < 0.35:\n",
    "                continue\n",
    "            else:\n",
    "                cost_matix[i, j] = 0\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matix)\n",
    "    fn = len(gt_boxes) - row_ind.shape[0]\n",
    "    fp = len(pred_boxes) - col_ind.shape[0]\n",
    "    tp = 0\n",
    "    for i, j in zip(row_ind, col_ind):\n",
    "        if cost_matix[i, j] == 0:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "            fn += 1\n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "def get_boxes(df):\n",
    "    arr = df[['frame', 'left', 'top', 'width', 'height']].to_numpy()\n",
    "    arr[:, 3] = arr[:,1] + arr[:, 3]\n",
    "    arr[:, 4] = arr[:,2] + arr[:, 4]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(test_df, val_df):\n",
    "    ftp, ffp, ffn = [], [], []\n",
    "    for vid in val_df.video.unique():\n",
    "        #if vid.split('_')[-1].split('.')[0] == 'Endzone':\n",
    "        #if vid.split('_')[-1].split('.')[0] == 'Sideline':\n",
    "        #    continue\n",
    "        pred_boxes = get_boxes(test_df[test_df.video == vid])\n",
    "        gt_boxes = get_boxes(val_df[(val_df.video == vid) & (val_df.impact == 2)])\n",
    "        tp, fp, fn = precision_calc(gt_boxes, pred_boxes)\n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        recall = tp / (tp + fn + 1e-6)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "        print(f'{vid}:  TP: {tp}, FP: {fp}, FN: {fn}, PRECISION: {precision:.2f}, RECALL: {recall:.2f}, F1 SCORE: {f1_score:.2f}')\n",
    "        ftp.append(tp)\n",
    "        ffp.append(fp)\n",
    "        ffn.append(fn)\n",
    "    tp = np.sum(ftp)\n",
    "    fp = np.sum(ffp)\n",
    "    fn = np.sum(ffn)\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    return tp, fp, fn, precision, recall, f1_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code for checking\n",
    "# check_video_df = test_df[test_df.video == '58095_004022_Endzone.mp4']\n",
    "# check_video_df = val_df[(val_df.video == '58095_004022_Endzone.mp4') & (val_df.impact == 2)]\n",
    "# check_video_df['frame'] = check_video_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_video_df = check_video_df[check_video_df.frame == 176]\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "# sample = cv2.cvtColor(cv2.imread('../data/train_images_all/58095_004022_Endzone_176.png'), cv2.COLOR_BGR2RGB)\n",
    "# for index, row in check_video_df.iterrows():\n",
    "#     print(index)\n",
    "#     x1 = row['left']\n",
    "#     y1 = row['top']\n",
    "#     x2 = x1 + row['width']\n",
    "#     y2 = y1 + row['height']\n",
    "    \n",
    "#     #cv2.rectangle(sample, (x1, y1), (x2, y2), (1, 0, 0), 3)\n",
    "# ax.set_axis_off()\n",
    "# ax.imshow(sample);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        scores                     image_name  detector_scores  \\\n",
       "0     0.026172     57583_000082_Endzone_8.png         0.300132   \n",
       "1     0.028701     57583_000082_Endzone_9.png         0.304256   \n",
       "2     0.163126    57583_000082_Endzone_18.png         0.345572   \n",
       "3     0.205627    57583_000082_Endzone_32.png         0.343757   \n",
       "4     0.216342    57583_000082_Endzone_32.png         0.328258   \n",
       "...        ...                            ...              ...   \n",
       "5937  0.212036  58103_003494_Sideline_188.png         0.357808   \n",
       "5938  0.177606  58103_003494_Sideline_189.png         0.360596   \n",
       "5939  0.035858  58103_003494_Sideline_190.png         0.303808   \n",
       "5940  0.044243  58103_003494_Sideline_191.png         0.348191   \n",
       "5941  0.181613  58103_003494_Sideline_226.png         0.302720   \n",
       "\n",
       "      classifier_scores  left  top  width  height  frame  \\\n",
       "0              0.087201   592  325     19      24      8   \n",
       "1              0.094332   591  325     20      24      9   \n",
       "2              0.472045   589  325     22      23     18   \n",
       "3              0.598176   564  328     23      21     32   \n",
       "4              0.659063   582  312     21      24     32   \n",
       "...                 ...   ...  ...    ...     ...    ...   \n",
       "5937           0.592598   736  386     17      18    188   \n",
       "5938           0.492535   737  387     16      17    189   \n",
       "5939           0.118029   776  303     17      16    190   \n",
       "5940           0.127065   776  301     17      16    191   \n",
       "5941           0.599937   689  241     21      24    226   \n",
       "\n",
       "                          video  \n",
       "0      57583_000082_Endzone.mp4  \n",
       "1      57583_000082_Endzone.mp4  \n",
       "2      57583_000082_Endzone.mp4  \n",
       "3      57583_000082_Endzone.mp4  \n",
       "4      57583_000082_Endzone.mp4  \n",
       "...                         ...  \n",
       "5937  58103_003494_Sideline.mp4  \n",
       "5938  58103_003494_Sideline.mp4  \n",
       "5939  58103_003494_Sideline.mp4  \n",
       "5940  58103_003494_Sideline.mp4  \n",
       "5941  58103_003494_Sideline.mp4  \n",
       "\n",
       "[5942 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>scores</th>\n      <th>image_name</th>\n      <th>detector_scores</th>\n      <th>classifier_scores</th>\n      <th>left</th>\n      <th>top</th>\n      <th>width</th>\n      <th>height</th>\n      <th>frame</th>\n      <th>video</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.026172</td>\n      <td>57583_000082_Endzone_8.png</td>\n      <td>0.300132</td>\n      <td>0.087201</td>\n      <td>592</td>\n      <td>325</td>\n      <td>19</td>\n      <td>24</td>\n      <td>8</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.028701</td>\n      <td>57583_000082_Endzone_9.png</td>\n      <td>0.304256</td>\n      <td>0.094332</td>\n      <td>591</td>\n      <td>325</td>\n      <td>20</td>\n      <td>24</td>\n      <td>9</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.163126</td>\n      <td>57583_000082_Endzone_18.png</td>\n      <td>0.345572</td>\n      <td>0.472045</td>\n      <td>589</td>\n      <td>325</td>\n      <td>22</td>\n      <td>23</td>\n      <td>18</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.205627</td>\n      <td>57583_000082_Endzone_32.png</td>\n      <td>0.343757</td>\n      <td>0.598176</td>\n      <td>564</td>\n      <td>328</td>\n      <td>23</td>\n      <td>21</td>\n      <td>32</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.216342</td>\n      <td>57583_000082_Endzone_32.png</td>\n      <td>0.328258</td>\n      <td>0.659063</td>\n      <td>582</td>\n      <td>312</td>\n      <td>21</td>\n      <td>24</td>\n      <td>32</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5937</th>\n      <td>0.212036</td>\n      <td>58103_003494_Sideline_188.png</td>\n      <td>0.357808</td>\n      <td>0.592598</td>\n      <td>736</td>\n      <td>386</td>\n      <td>17</td>\n      <td>18</td>\n      <td>188</td>\n      <td>58103_003494_Sideline.mp4</td>\n    </tr>\n    <tr>\n      <th>5938</th>\n      <td>0.177606</td>\n      <td>58103_003494_Sideline_189.png</td>\n      <td>0.360596</td>\n      <td>0.492535</td>\n      <td>737</td>\n      <td>387</td>\n      <td>16</td>\n      <td>17</td>\n      <td>189</td>\n      <td>58103_003494_Sideline.mp4</td>\n    </tr>\n    <tr>\n      <th>5939</th>\n      <td>0.035858</td>\n      <td>58103_003494_Sideline_190.png</td>\n      <td>0.303808</td>\n      <td>0.118029</td>\n      <td>776</td>\n      <td>303</td>\n      <td>17</td>\n      <td>16</td>\n      <td>190</td>\n      <td>58103_003494_Sideline.mp4</td>\n    </tr>\n    <tr>\n      <th>5940</th>\n      <td>0.044243</td>\n      <td>58103_003494_Sideline_191.png</td>\n      <td>0.348191</td>\n      <td>0.127065</td>\n      <td>776</td>\n      <td>301</td>\n      <td>17</td>\n      <td>16</td>\n      <td>191</td>\n      <td>58103_003494_Sideline.mp4</td>\n    </tr>\n    <tr>\n      <th>5941</th>\n      <td>0.181613</td>\n      <td>58103_003494_Sideline_226.png</td>\n      <td>0.302720</td>\n      <td>0.599937</td>\n      <td>689</td>\n      <td>241</td>\n      <td>21</td>\n      <td>24</td>\n      <td>226</td>\n      <td>58103_003494_Sideline.mp4</td>\n    </tr>\n  </tbody>\n</table>\n<p>5942 rows Ã— 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMS_THRESHOLD = 0.35\n",
    "CLS_THRESHOLD = 0.89\n",
    "DETECTOR_THRESHOLD = 0.38\n",
    "NMS_PARAM = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nmax_f1_score = 0\\nfor CLS_THRESHOLD in tqdm.tqdm(np.array((range(800,1000,10))) / 1000):\\n    for DETECTOR_THRESHOLD in np.array((range(350,550,10))) / 1000:\\n        for NMS_THRESHOLD in np.array((range(350,850,50))) / 1000:\\n            test_df_preditions = test_df[test_df.classifier_scores>CLS_THRESHOLD][test_df.detector_scores>DETECTOR_THRESHOLD]\\n            test_df_filtered_by_nms = filter_by_nms(test_df_preditions, NMS_THRESHOLD, 5000)\\n            tp, fp, fn, precision, recall, f1_score = calculate_metrics(test_df_filtered_by_nms, val_df)\\n            if f1_score>max_f1_score:\\n                max_f1_score=f1_score\\n                save_DETECTOR_THRESHOLD = DETECTOR_THRESHOLD\\n                save_CLS_THRESHOLD = CLS_THRESHOLD\\n                save_NMS_THRESHOLD = NMS_THRESHOLD\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "# CODE FOR GRID SEARCH\n",
    "\"\"\"\n",
    "max_f1_score = 0\n",
    "for CLS_THRESHOLD in tqdm.tqdm(np.array((range(800,1000,10))) / 1000):\n",
    "    for DETECTOR_THRESHOLD in np.array((range(350,550,10))) / 1000:\n",
    "        for NMS_THRESHOLD in np.array((range(350,850,50))) / 1000:\n",
    "            test_df_preditions = test_df[test_df.classifier_scores>CLS_THRESHOLD][test_df.detector_scores>DETECTOR_THRESHOLD]\n",
    "            test_df_filtered_by_nms = filter_by_nms(test_df_preditions, NMS_THRESHOLD, 5000)\n",
    "            tp, fp, fn, precision, recall, f1_score = calculate_metrics(test_df_filtered_by_nms, val_df)\n",
    "            if f1_score>max_f1_score:\n",
    "                max_f1_score=f1_score\n",
    "                save_DETECTOR_THRESHOLD = DETECTOR_THRESHOLD\n",
    "                save_CLS_THRESHOLD = CLS_THRESHOLD\n",
    "                save_NMS_THRESHOLD = NMS_THRESHOLD\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        scores                     image_name  detector_scores  \\\n",
       "6     0.247238    57583_000082_Endzone_44.png         0.276593   \n",
       "7     0.287932    57583_000082_Endzone_45.png         0.311507   \n",
       "8     0.228267    57583_000082_Endzone_45.png         0.253990   \n",
       "9     0.303664    57583_000082_Endzone_46.png         0.329177   \n",
       "10    0.189574    57583_000082_Endzone_46.png         0.210522   \n",
       "...        ...                            ...              ...   \n",
       "1881  0.196533   58103_003494_Sideline_63.png         0.213840   \n",
       "1882  0.193788   58103_003494_Sideline_65.png         0.211032   \n",
       "1894  0.199695   58103_003494_Sideline_82.png         0.223266   \n",
       "1897  0.192379  58103_003494_Sideline_114.png         0.212519   \n",
       "1898  0.183936  58103_003494_Sideline_114.png         0.202260   \n",
       "\n",
       "      classifier_scores  left  top  width  height  frame  \\\n",
       "6              0.893869   420  311     21      26     44   \n",
       "7              0.924318   423  312     20      24     45   \n",
       "8              0.898725   445  322     18      13     45   \n",
       "9              0.922495   426  313     19      22     46   \n",
       "10             0.900497   443  320     18      14     46   \n",
       "...                 ...   ...  ...    ...     ...    ...   \n",
       "1881           0.919065   743  346     15      16     63   \n",
       "1882           0.918286   741  347     15      16     65   \n",
       "1894           0.894428   878  173     17      18     82   \n",
       "1897           0.905234   768  191     15      16    114   \n",
       "1898           0.909407   778  191     14      15    114   \n",
       "\n",
       "                          video  \n",
       "6      57583_000082_Endzone.mp4  \n",
       "7      57583_000082_Endzone.mp4  \n",
       "8      57583_000082_Endzone.mp4  \n",
       "9      57583_000082_Endzone.mp4  \n",
       "10     57583_000082_Endzone.mp4  \n",
       "...                         ...  \n",
       "1881  58103_003494_Sideline.mp4  \n",
       "1882  58103_003494_Sideline.mp4  \n",
       "1894  58103_003494_Sideline.mp4  \n",
       "1897  58103_003494_Sideline.mp4  \n",
       "1898  58103_003494_Sideline.mp4  \n",
       "\n",
       "[672 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>scores</th>\n      <th>image_name</th>\n      <th>detector_scores</th>\n      <th>classifier_scores</th>\n      <th>left</th>\n      <th>top</th>\n      <th>width</th>\n      <th>height</th>\n      <th>frame</th>\n      <th>video</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>0.247238</td>\n      <td>57583_000082_Endzone_44.png</td>\n      <td>0.276593</td>\n      <td>0.893869</td>\n      <td>420</td>\n      <td>311</td>\n      <td>21</td>\n      <td>26</td>\n      <td>44</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.287932</td>\n      <td>57583_000082_Endzone_45.png</td>\n      <td>0.311507</td>\n      <td>0.924318</td>\n      <td>423</td>\n      <td>312</td>\n      <td>20</td>\n      <td>24</td>\n      <td>45</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.228267</td>\n      <td>57583_000082_Endzone_45.png</td>\n      <td>0.253990</td>\n      <td>0.898725</td>\n      <td>445</td>\n      <td>322</td>\n      <td>18</td>\n      <td>13</td>\n      <td>45</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.303664</td>\n      <td>57583_000082_Endzone_46.png</td>\n      <td>0.329177</td>\n      <td>0.922495</td>\n      <td>426</td>\n      <td>313</td>\n      <td>19</td>\n      <td>22</td>\n      <td>46</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.189574</td>\n      <td>57583_000082_Endzone_46.png</td>\n      <td>0.210522</td>\n      <td>0.900497</td>\n      <td>443</td>\n      <td>320</td>\n      <td>18</td>\n      <td>14</td>\n      <td>46</td>\n      <td>57583_000082_Endzone.mp4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1881</th>\n      <td>0.196533</td>\n      <td>58103_003494_Sideline_63.png</td>\n      <td>0.213840</td>\n      <td>0.919065</td>\n      <td>743</td>\n      <td>346</td>\n      <td>15</td>\n      <td>16</td>\n      <td>63</td>\n      <td>58103_003494_Sideline.mp4</td>\n    </tr>\n    <tr>\n      <th>1882</th>\n      <td>0.193788</td>\n      <td>58103_003494_Sideline_65.png</td>\n      <td>0.211032</td>\n      <td>0.918286</td>\n      <td>741</td>\n      <td>347</td>\n      <td>15</td>\n      <td>16</td>\n      <td>65</td>\n      <td>58103_003494_Sideline.mp4</td>\n    </tr>\n    <tr>\n      <th>1894</th>\n      <td>0.199695</td>\n      <td>58103_003494_Sideline_82.png</td>\n      <td>0.223266</td>\n      <td>0.894428</td>\n      <td>878</td>\n      <td>173</td>\n      <td>17</td>\n      <td>18</td>\n      <td>82</td>\n      <td>58103_003494_Sideline.mp4</td>\n    </tr>\n    <tr>\n      <th>1897</th>\n      <td>0.192379</td>\n      <td>58103_003494_Sideline_114.png</td>\n      <td>0.212519</td>\n      <td>0.905234</td>\n      <td>768</td>\n      <td>191</td>\n      <td>15</td>\n      <td>16</td>\n      <td>114</td>\n      <td>58103_003494_Sideline.mp4</td>\n    </tr>\n    <tr>\n      <th>1898</th>\n      <td>0.183936</td>\n      <td>58103_003494_Sideline_114.png</td>\n      <td>0.202260</td>\n      <td>0.909407</td>\n      <td>778</td>\n      <td>191</td>\n      <td>14</td>\n      <td>15</td>\n      <td>114</td>\n      <td>58103_003494_Sideline.mp4</td>\n    </tr>\n  </tbody>\n</table>\n<p>672 rows Ã— 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 208
    }
   ],
   "source": [
    "test_df_preditions = test_df[test_df.classifier_scores>CLS_THRESHOLD][test_df.detector_scores>DETECTOR_THRESHOLD]\n",
    "test_df_preditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "57583_000082_Endzone.mp4:  TP: 3, FP: 22, FN: 16, PRECISION: 0.12, RECALL: 0.16, F1 SCORE: 0.14\n",
      "57583_000082_Sideline.mp4:  TP: 2, FP: 22, FN: 16, PRECISION: 0.08, RECALL: 0.11, F1 SCORE: 0.10\n",
      "57586_004152_Endzone.mp4:  TP: 6, FP: 22, FN: 2, PRECISION: 0.21, RECALL: 0.75, F1 SCORE: 0.33\n",
      "57586_004152_Sideline.mp4:  TP: 4, FP: 23, FN: 5, PRECISION: 0.15, RECALL: 0.44, F1 SCORE: 0.22\n",
      "57679_003316_Endzone.mp4:  TP: 7, FP: 56, FN: 12, PRECISION: 0.11, RECALL: 0.37, F1 SCORE: 0.17\n",
      "57679_003316_Sideline.mp4:  TP: 3, FP: 3, FN: 14, PRECISION: 0.50, RECALL: 0.18, F1 SCORE: 0.26\n",
      "57680_002206_Endzone.mp4:  TP: 12, FP: 55, FN: 7, PRECISION: 0.18, RECALL: 0.63, F1 SCORE: 0.28\n",
      "57680_002206_Sideline.mp4:  TP: 8, FP: 71, FN: 11, PRECISION: 0.10, RECALL: 0.42, F1 SCORE: 0.16\n",
      "57906_000718_Endzone.mp4:  TP: 5, FP: 35, FN: 15, PRECISION: 0.12, RECALL: 0.25, F1 SCORE: 0.17\n",
      "57906_000718_Sideline.mp4:  TP: 2, FP: 15, FN: 14, PRECISION: 0.12, RECALL: 0.12, F1 SCORE: 0.12\n",
      "57911_000147_Endzone.mp4:  TP: 4, FP: 48, FN: 17, PRECISION: 0.08, RECALL: 0.19, F1 SCORE: 0.11\n",
      "57911_000147_Sideline.mp4:  TP: 4, FP: 14, FN: 10, PRECISION: 0.22, RECALL: 0.29, F1 SCORE: 0.25\n",
      "57997_003691_Endzone.mp4:  TP: 9, FP: 58, FN: 21, PRECISION: 0.13, RECALL: 0.30, F1 SCORE: 0.19\n",
      "57997_003691_Sideline.mp4:  TP: 6, FP: 14, FN: 21, PRECISION: 0.30, RECALL: 0.22, F1 SCORE: 0.26\n",
      "57998_002181_Endzone.mp4:  TP: 4, FP: 25, FN: 12, PRECISION: 0.14, RECALL: 0.25, F1 SCORE: 0.18\n",
      "57998_002181_Sideline.mp4:  TP: 1, FP: 64, FN: 15, PRECISION: 0.02, RECALL: 0.06, F1 SCORE: 0.02\n",
      "58005_001254_Endzone.mp4:  TP: 1, FP: 43, FN: 6, PRECISION: 0.02, RECALL: 0.14, F1 SCORE: 0.04\n",
      "58005_001254_Sideline.mp4:  TP: 0, FP: 17, FN: 5, PRECISION: 0.00, RECALL: 0.00, F1 SCORE: 0.00\n",
      "58048_000086_Endzone.mp4:  TP: 1, FP: 19, FN: 16, PRECISION: 0.05, RECALL: 0.06, F1 SCORE: 0.05\n",
      "58048_000086_Sideline.mp4:  TP: 4, FP: 28, FN: 13, PRECISION: 0.12, RECALL: 0.24, F1 SCORE: 0.16\n",
      "58095_004022_Endzone.mp4:  TP: 0, FP: 14, FN: 17, PRECISION: 0.00, RECALL: 0.00, F1 SCORE: 0.00\n",
      "58095_004022_Sideline.mp4:  TP: 2, FP: 3, FN: 12, PRECISION: 0.40, RECALL: 0.14, F1 SCORE: 0.21\n",
      "58103_003494_Endzone.mp4:  TP: 4, FP: 29, FN: 17, PRECISION: 0.12, RECALL: 0.19, F1 SCORE: 0.15\n",
      "58103_003494_Sideline.mp4:  TP: 5, FP: 5, FN: 7, PRECISION: 0.50, RECALL: 0.42, F1 SCORE: 0.45\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(97, 705, 301, 0.12094763077188575, 0.24371859235246585, 0.16166622307065998)"
      ]
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "source": [
    "#TTA horizontal and channel flip\n",
    "calculate_metrics(test_df_preditions, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_filtered_by_nms = filter_by_nms(test_df_preditions, NMS_THRESHOLD, NMS_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "57583_000082_Endzone.mp4:  TP: 3, FP: 4, FN: 16, PRECISION: 0.43, RECALL: 0.16, F1 SCORE: 0.23\n",
      "57583_000082_Sideline.mp4:  TP: 1, FP: 6, FN: 17, PRECISION: 0.14, RECALL: 0.06, F1 SCORE: 0.08\n",
      "57586_004152_Endzone.mp4:  TP: 5, FP: 3, FN: 3, PRECISION: 0.62, RECALL: 0.62, F1 SCORE: 0.62\n",
      "57586_004152_Sideline.mp4:  TP: 3, FP: 1, FN: 6, PRECISION: 0.75, RECALL: 0.33, F1 SCORE: 0.46\n",
      "57679_003316_Endzone.mp4:  TP: 7, FP: 5, FN: 12, PRECISION: 0.58, RECALL: 0.37, F1 SCORE: 0.45\n",
      "57679_003316_Sideline.mp4:  TP: 3, FP: 1, FN: 14, PRECISION: 0.75, RECALL: 0.18, F1 SCORE: 0.29\n",
      "57680_002206_Endzone.mp4:  TP: 10, FP: 14, FN: 9, PRECISION: 0.42, RECALL: 0.53, F1 SCORE: 0.47\n",
      "57680_002206_Sideline.mp4:  TP: 5, FP: 7, FN: 14, PRECISION: 0.42, RECALL: 0.26, F1 SCORE: 0.32\n",
      "57906_000718_Endzone.mp4:  TP: 5, FP: 11, FN: 15, PRECISION: 0.31, RECALL: 0.25, F1 SCORE: 0.28\n",
      "57906_000718_Sideline.mp4:  TP: 2, FP: 5, FN: 14, PRECISION: 0.29, RECALL: 0.12, F1 SCORE: 0.17\n",
      "57911_000147_Endzone.mp4:  TP: 3, FP: 9, FN: 18, PRECISION: 0.25, RECALL: 0.14, F1 SCORE: 0.18\n",
      "57911_000147_Sideline.mp4:  TP: 1, FP: 3, FN: 13, PRECISION: 0.25, RECALL: 0.07, F1 SCORE: 0.11\n",
      "57997_003691_Endzone.mp4:  TP: 6, FP: 9, FN: 24, PRECISION: 0.40, RECALL: 0.20, F1 SCORE: 0.27\n",
      "57997_003691_Sideline.mp4:  TP: 4, FP: 4, FN: 23, PRECISION: 0.50, RECALL: 0.15, F1 SCORE: 0.23\n",
      "57998_002181_Endzone.mp4:  TP: 4, FP: 7, FN: 12, PRECISION: 0.36, RECALL: 0.25, F1 SCORE: 0.30\n",
      "57998_002181_Sideline.mp4:  TP: 1, FP: 16, FN: 15, PRECISION: 0.06, RECALL: 0.06, F1 SCORE: 0.06\n",
      "58005_001254_Endzone.mp4:  TP: 1, FP: 11, FN: 6, PRECISION: 0.08, RECALL: 0.14, F1 SCORE: 0.11\n",
      "58005_001254_Sideline.mp4:  TP: 0, FP: 4, FN: 5, PRECISION: 0.00, RECALL: 0.00, F1 SCORE: 0.00\n",
      "58048_000086_Endzone.mp4:  TP: 1, FP: 5, FN: 16, PRECISION: 0.17, RECALL: 0.06, F1 SCORE: 0.09\n",
      "58048_000086_Sideline.mp4:  TP: 4, FP: 7, FN: 13, PRECISION: 0.36, RECALL: 0.24, F1 SCORE: 0.29\n",
      "58095_004022_Endzone.mp4:  TP: 0, FP: 6, FN: 17, PRECISION: 0.00, RECALL: 0.00, F1 SCORE: 0.00\n",
      "58095_004022_Sideline.mp4:  TP: 1, FP: 1, FN: 13, PRECISION: 0.50, RECALL: 0.07, F1 SCORE: 0.12\n",
      "58103_003494_Endzone.mp4:  TP: 3, FP: 5, FN: 18, PRECISION: 0.37, RECALL: 0.14, F1 SCORE: 0.21\n",
      "58103_003494_Sideline.mp4:  TP: 5, FP: 0, FN: 7, PRECISION: 1.00, RECALL: 0.42, F1 SCORE: 0.59\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(78, 144, 320, 0.3513513497686876, 0.19597989900507562, 0.25161244270635474)"
      ]
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "source": [
    "calculate_metrics(test_df_filtered_by_nms, val_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.4 64-bit ('nfl')",
   "metadata": {
    "interpreter": {
     "hash": "e30599d7f353069b7d48b33cea74e2cf40593e9b87e2b3720e2cc41af1fc546b"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}